project_name: scaling
group_name: performance_benchmark
algorithm_name: rmappo
env_name: StarCraft2
map_name: 27m_vs_30m
user_name: garrett4wade
experiment_name: benchmark_27m_vs_30m
benchmark: false
cuda: true
cuda_deterministic: true
seed: 1
no_summary: false
use_wandb: true
use_eval: true
use_render: false

num_env_steps: 200000000
train_for_env_steps: 200000000
train_for_seconds: 10000000000000000
model_dir: null
save_dir: './models'
summary_dir: './logs'

# system parameters
num_policies: 1
num_tasks_per_node: 1
learner_config:
  "0":
    "0": 0
    # "1": 0
    # "2": 0
    # "3": 0
policy2agents:
  "0":
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23
    - 24
    - 25
    - 26
ddp_init_methods:
  - tcp://10.0.4.132:6666
task_dispatcher_addr: tcp://10.0.4.132:8887
task_result_addr: tcp://10.0.4.132:8886
model_weights_addrs:  # every learner has a model_weights_addr and len(model_weights_adddrs) == num_policies == num_leraners
  - tcp://10.0.4.132:8889
seg_addrs:  # every worker node has seg_addrs connecting to all learners
  - - tcp://10.0.4.132:12346
num_actors: 48
actor_group_size: 1
envs_per_actor: 2
min_num_requests: -1
num_policy_workers: 4
slots_per_update: 48
qsize: 12
num_splits: 2
data_chunk_length: 10
episode_length: 400

# fixed system parameters
stats_avg: 100
set_workers_cpu_affinity: true
force_envs_single_thread: true
default_niceness: 0
eval_episodes: 32
eval_interval: 100
log_interval: 20
n_training_threads: 16
reset_timeout_seconds: 60
actor_worker_gpus: 0

# algorithm parameters
hidden_size: 64
layer_N: 1
rec_n: 1
sample_reuse: 5
num_mini_batch: 1
lr: 0.0005
max_grad_norm: 10.0
entropy_coef: 0.01
gamma: 0.99
gae_lambda: 0.95
gain: 0.01
value_coef: 1.0
weight_decay: 0
opti_eps: 1.0e-05
clip_param: 0.2
huber_delta: 10.0

# render parameters
ifi: 0.1
render_episodes: 5
save_gifs: false
save_interval: 1

# tricks
share_policy: true
use_reanalyze: false
use_fct_masks: true
use_feature_normalization: true
use_gae: true
use_huber_loss: true
use_linear_lr_decay: false
use_max_grad_norm: true
use_orthogonal: false
use_popart: true
use_proper_time_limits: false
use_ReLU: true
use_active_masks: true
use_advantage_normalization: true
use_centralized_V: true
use_clipped_value_loss: true

# some fixed tricks
add_agent_id: false
add_center_xy: true
add_distance_state: false
add_enemy_action_state: false
add_local_obs: false
add_move_state: false
add_visible_state: false
add_xy_state: false
use_stacked_frames: false
stacked_frames: 1
use_state_agent: true
use_mustalive: true
use_obs_instead_of_state: false

# setup configs, use absolute dir
cwd: "/workspace/workspace/scaling_marl"
trainer_dir: "run_learner_node.py"
worker_dir: "run_worker_node_on_smac.py"
monitor_dir: "run_monitor.py"
log_dir: "log/"

trainer_logname: "trainer.log"
worker_logname: "worker.log"
monitor_logname: "monitor.log"

# configdir: "configs/starcraft2/config.yaml"
workers: "71"
trainers: "70"
container_name: scaling

# monitor configs
total_rounds: 100000000 # debug
interval: 3
# first node is head
nodes: learner0,worker0
puller_address: "10.0.4.132"
puller_port: 4321

verbose: false
